{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "PS/2019/246\n",
        "W.A.S.H PERERA\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_kiFbu2YCLHX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Importing Relevant Libraries"
      ],
      "metadata": {
        "id": "Id2OtRd8D0mM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preparing Data"
      ],
      "metadata": {
        "id": "2MCnUnbdyrjd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import tensorflow as tf\n",
        "import string\n",
        "import re\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "metadata": {
        "id": "ZCX9LPNMEAVZ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Mounting the Google Drive"
      ],
      "metadata": {
        "id": "Dsk5UXYkEhNK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Htq7RUVDEpJl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0afe2cdd-cc35-4b56-c5a2-400b85bc6134"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Reading the Data File"
      ],
      "metadata": {
        "id": "FMxmuWMVIEBE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_file = \"/content/drive/My Drive/Colab Notebooks/Content.txt\"\n",
        "with open(text_file) as f:  # Opening the file in read mode\n",
        "    lines = f.read().split(\"\\n\")[:-1]   # Reading the lines from the file and splitting by newline character"
      ],
      "metadata": {
        "id": "5bBeA8hlIJJW"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the first 20 lines of the file\n",
        "i = 0\n",
        "for line in lines:\n",
        "  print(line)\n",
        "  i = i + 1\n",
        "  if(i==20):\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fe5mXUuF4zUR",
        "outputId": "4ed81c9b-3479-46be-a2e5-f93d194c46f6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the water was falling drop by drop\tවතුර බිංදුවෙන් බිංදුව වැටෙනවා\n",
            "the water had started a fire\tවතුරෙන් ගින්නක් ඇවිළුණා\n",
            "then my heart was remembering you\tමගේ හදවත ඔයාව සිහිපත් කළා\n",
            "beloved now you only tell me\tසොදුරියේ දැන් මට කියන්න\n",
            "what should i do\tමම මොනවද කරන්න ඕනේ\n",
            "without you i can't live anymore\tඔයා නැතුව මට තවත් ජීවත් වෙන්න බෑ\n",
            "without you what is my existence\tඔයා නැතුව මගේ පැවැත්ම මොකක්ද\n",
            "then i'll be separated from myself\tඊට පස්සේ මම මාවම වෙන් කර ගන්නවා\n",
            "i live every day for you\tමම හැමදාම ඔයා වෙනුවෙන් ජීවත් වෙනවා\n",
            "if i become yours\tමම ඔයාගේ වුනොත්\n",
            "because you are the one\tමොකද ඔයා තමයි\n",
            "now you are the one\tදැන් ඔයා තමයි\n",
            "you are my life now\tඔයා තමයි දැන් මගේ ජීවිතය\n",
            "both peace and pain my love are now only you\tසාමය සහ වේදනාව මගේ ආදරණීය දැන් ඔබ පමණයි\n",
            "what kind of relationship is ours\tමොන වගේ සම්බන්ධතාවයක්ද අපේ\n",
            "i cannot bear a moment without you\tමට ඔයා නැතුව මොහොතක්වත් ඉන්න බෑ\n",
            "i have given you all my time\tමම ඔයාට මගේ මුළු කාලයම දුන්නා\n",
            "your name is on every breath i take\tමම ගන්න හැම හුස්මකම ඔයාගේ නම තියෙනවා\n",
            "thank you for your understanding\tඔයාගේ තේරුම් ගැනීමට ස්තූතියි\n",
            "if i get separated from you\tමම ඔයාගෙන් වෙන් වුනොත්\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the last 10 lines of the file\n",
        "for x in range(len(lines)-10,len(lines)):\n",
        "  print(lines[x])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bRAXkb038IK0",
        "outputId": "be1a4baf-c66e-4f03-869e-7da903596953"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Once upon a time in a small village there lived a young girl named Lily who had a special gift for growing the most beautiful flowers\tඑක් කලෙක කුඩා ගමක ලිලී නම් තරුණියක් ජීවත් වූ අතර ඇයට ලස්සනම මල් වගා කිරීම සඳහා විශේෂ තෑග්ගක් තිබුණි\n",
            "In a distant land a brave knight named Sir Arthur embarked on a perilous journey to rescue a kidnapped princess from an evil sorcerer\tදුර රටක ශ්‍රීමත් ආතර් නම් නිර්භීත නයිට්වරයා නපුරු මායාකාරයෙකුගෙන් පැහැරගෙන ගිය කුමරියක් බේරා ගැනීමට භයානක ගමනක් ආරම්භ කළේය\n",
            "Emily a curious explorer set out on an adventure to uncover the hidden treasures of an ancient lost city deep in the Amazon rainforest\tකුතුහලය දනවන ගවේෂකයෙකු වන එමිලි ඇමසන් වනාන්තරයේ ගැඹුරින් ගිලිහී ගිය පැරණි නගරයක සැඟවුණු වස්තු සොයා ගැනීමට ත්‍රාසජනක ගමනක් ආරම්භ කළාය\n",
            "In the peaceful town of Willowbrook a mischievous cat named Oliver had a talent for getting into amusing and unexpected predicaments\tසාමකාමී නගරයක් වන විලෝබෲක්හි ඔලිවර් නම් දඟකාර බළලාට විනෝදජනක සහ අනපේක්ෂිත දුෂ්කරතාවන්ට පත්වීමේ දක්ෂතාවයක් තිබුණි\n",
            "Sarah a talented pianist dreamt of performing on the grand stage of Carnegie Hall and spent countless hours practicing her musical craft\tදක්ෂ පියානෝ වාදකයෙකු වන සාරා Carnegie Hall හි මහා වේදිකාවේ රඟ දැක්වීමට සිහින මැවූ අතර ඇගේ සංගීත ශිල්පය පුහුණු කිරීමට පැය ගණන් ගත කළාය\n",
            "Deep in the enchanted forest a group of woodland creatures led by a wise old owl embarked on a quest to save their home from destruction\tවශීකෘත වනාන්තරයේ ගැඹුරින් නුවණැති මහලු බකමූණෙකු විසින් මෙහෙයවන ලද වනාන්තර ජීවීන් කණ්ඩායමක් තම නිවස විනාශයෙන් බේරා ගැනීමේ ගවේෂණයක යෙදී සිටියහ\n",
            "On a sunny summer day a group of friends gathered at the beach for a fun-filled day of sandcastle building swimming and laughter\tඅව්ව සහිත ගිම්හාන දිනයක මිතුරන් පිරිසක් වැලි මාලිගා ගොඩ නැගීම පිහිනීම සහ සිනහවෙන් විනෝදයෙන් පිරුණු දවසක් සඳහා වෙරළට රැස් වූහ\n",
            "In a quaint seaside village a mysterious stranger arrived bringing with them an air of excitement and an unexpected twist to the townspeople's lives\tවිචිත්‍රවත් මුහුදු වෙරළේ ගම්මානයකට අද්භූත ආගන්තුකයෙකු පැමිණියේ ඔවුන් සමඟ උද්දීපනයක් සහ නගර වැසියන්ගේ ජීවිතයට අනපේක්ෂිත පෙරළියක් ගෙන එයි\n",
            "Thomas an aspiring writer found inspiration in the bustling streets of a vibrant city where every corner held a story waiting to be told\tඅභිලාෂකාමී ලේඛකයෙකු වන තෝමස් සෑම අස්සක් මුල්ලක් නෑරම කීමට බලා සිටින කතන්දර ඇති උද්යෝගිමත් නගරයක කාර්යබහුල වීදිවල ආශ්වාදයක් ලබා ගත්තේය\n",
            "In a land of mythical creatures a young dragon named Ember struggled to control her fiery breath while learning valuable lessons of friendship and courage\tමිථ්‍යා ජීවීන් සිටින රටක එම්බර් නම් තරුණ මකරෙක් මිත්‍රත්වයේ සහ ධෛර්‍යයේ වටිනා පාඩම් ඉගෙන ගනිමින් ඇගේ ගිනි හුස්ම පාලනය කිරීමට මහත් පරිශ්‍රමයක් දැරීය\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Spliting the English and Sinhala Translation Pairs"
      ],
      "metadata": {
        "id": "sETT7okV9H0I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_pairs = []\n",
        "for line in lines:\n",
        "    english, sinhala = line.split(\"\\t\") # Split the line by tab to separate English and Sinhala translations\n",
        "    sinhala = \"[start] \" + sinhala + \" [end]\" # Add start and end tokens to the Sinhala translation\n",
        "    text_pairs.append((english, sinhala))  # Append the English-Sinhalah pair to text_pairs list\n"
      ],
      "metadata": {
        "id": "dEJRl20G9SWy"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print 3 randomly chosen pairs\n",
        "for i in range(3):\n",
        "    print(random.choice(text_pairs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpVxOH2g98La",
        "outputId": "af71758c-d494-42d7-90db-114199b9562b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(\"so if you don't mind asking\", '[start] ඉතිං ඇහුවාට කමක් නැත්නම් [end]')\n",
            "(\"do you have the dog's certificates  \", '[start] ඔයා ළඟ බල්ලාගේ සහතික තියෙනවාද    [end]')\n",
            "('give it a minute', '[start] මිනිත්තුවක් දෙන්න [end]')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Randomizing the Data"
      ],
      "metadata": {
        "id": "4oEu4Xms-JVa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "random.shuffle(text_pairs)"
      ],
      "metadata": {
        "id": "vnr1eNSX-OBl"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Spliting the Data into Training, Validation and Testing"
      ],
      "metadata": {
        "id": "iHr0yjDN-avr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_val_samples = int(0.15 * len(text_pairs))  # Calculate the number of validation samples\n",
        "num_train_samples = len(text_pairs) - 2 * num_val_samples  # Calculate the number of training samples\n",
        "train_pairs = text_pairs[:num_train_samples]  # Assign the first part of shuffled pairs to training set\n",
        "val_pairs = text_pairs[num_train_samples:num_train_samples + num_val_samples]  # Assign the next part to validation set\n",
        "test_pairs = text_pairs[num_train_samples + num_val_samples:]  # Assign the rest to testing set"
      ],
      "metadata": {
        "id": "8ZBrAa2Q-gcv"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print sizes of each set\n",
        "print(\"Total sentences:\",len(text_pairs))\n",
        "print(\"Training set size:\",len(train_pairs))\n",
        "print(\"Validation set size:\",len(val_pairs))\n",
        "print(\"Testing set size:\",len(test_pairs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LNMixvPX--P9",
        "outputId": "26f0b3de-99db-497d-828f-60367480ba51"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total sentences: 80684\n",
            "Training set size: 56480\n",
            "Validation set size: 12102\n",
            "Testing set size: 12102\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_pairs)+len(val_pairs)+len(test_pairs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QCvUYAKU_by1",
        "outputId": "9b685c28-7a9b-42d6-badf-35cf8bacf882"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "80684"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Removing Punctuations"
      ],
      "metadata": {
        "id": "Neg9yBC__pXr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "strip_chars = string.punctuation + \"¿\"  # Define a string containing punctuation marks and \"¿\"\n",
        "strip_chars = strip_chars.replace(\"[\", \"\")  # Remove \"[\" character from strip_chars\n",
        "strip_chars = strip_chars.replace(\"]\", \"\")  # Remove \"]\" character from strip_chars\n",
        "\n",
        "# Print regex pattern for stripping punctuation marks\n",
        "f\"[{re.escape(strip_chars)}]\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "m9t8POwa_tM0",
        "outputId": "96b65828-da65-4aab-f02b-75ca25fea007"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'[!\"\\\\#\\\\$%\\\\&\\'\\\\(\\\\)\\\\*\\\\+,\\\\-\\\\./:;<=>\\\\?@\\\\\\\\\\\\^_`\\\\{\\\\|\\\\}\\\\~¿]'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vectorizing the English and Sinhala Test Pairs"
      ],
      "metadata": {
        "id": "2BE5ZuOsAbaD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_standardization(input_string):\n",
        "    lowercase = tf.strings.lower(input_string)  # Convert input string to lowercase\n",
        "    return tf.strings.regex_replace(\n",
        "        lowercase, f\"[{re.escape(strip_chars)}]\", \"\")  # Remove punctuation marks from the string\n",
        "\n",
        "vocab_size = 15000  # Define the vocabulary size\n",
        "sequence_length = 20  # Define the sequence length\n",
        "\n",
        "# Initialize TextVectorization layers for source (English) and target (Sinhala) texts\n",
        "source_vectorization = layers.TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length,\n",
        ")\n",
        "target_vectorization = layers.TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length + 1,\n",
        "    standardize=custom_standardization,\n",
        ")\n",
        "\n",
        "# Extract English and Sinhala texts from train_pairs\n",
        "train_english_texts = [pair[0] for pair in train_pairs]\n",
        "train_sinhala_texts = [pair[1] for pair in train_pairs]\n",
        "\n",
        "# Adapt the source vectorization layer to the English texts\n",
        "source_vectorization.adapt(train_english_texts)\n",
        "# Adapt the target vectorization layer to the Sinhala texts.\n",
        "target_vectorization.adapt(train_sinhala_texts)"
      ],
      "metadata": {
        "id": "GzXy7bYsAlDh"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Preparing Datasets for the Translation Task"
      ],
      "metadata": {
        "id": "gZspmPXpCf37"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64  # Define batch size\n",
        "\n",
        "def format_dataset(eng, sin):\n",
        "    eng = source_vectorization(eng)  # Vectorize English texts\n",
        "    sin = target_vectorization(sin)  # Vectorize Sinhala texts\n",
        "    max_length = tf.maximum(tf.shape(eng)[1], tf.shape(sin)[1])  # Get the maximum sequence length\n",
        "    eng = tf.pad(eng, [[0, 0], [0, max_length - tf.shape(eng)[1]]])[:, :max_length]  # Pad or truncate English sequences\n",
        "    sin = tf.pad(sin, [[0, 0], [0, max_length - tf.shape(sin)[1]]])[:, :max_length]  # Pad or truncate Sinhala sequences\n",
        "    return ({\n",
        "        \"english\": eng[:, :-1],\n",
        "        \"sinhala\": sin[:, :-1],\n",
        "    }, sin[:, 1:])  # Return formatted dataset with English and Sinhala inputs, and shifted Sinhala targets\n",
        "\n",
        "def make_dataset(pairs):\n",
        "    eng_texts, sin_texts = zip(*pairs)  # Unzip English-Sinhala pairs\n",
        "    eng_texts = list(eng_texts)  # Convert to list\n",
        "    sin_texts = list(sin_texts)  # Convert to list\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, sin_texts))  # Create dataset from tensors\n",
        "    dataset = dataset.batch(batch_size)  # Batch the dataset\n",
        "    dataset = dataset.map(format_dataset, num_parallel_calls=4)  # Map format_dataset function to the dataset\n",
        "    return dataset.shuffle(2048).prefetch(16).cache()  # Shuffle, prefetch, and cache the dataset\n",
        "\n",
        "# Create training and validation datasets\n",
        "train_ds = make_dataset(train_pairs)\n",
        "val_ds = make_dataset(val_pairs)\n",
        "\n",
        "# Print shapes of inputs and targets from the first batch of training dataset\n",
        "for inputs, targets in train_ds.take(1):\n",
        "    print(f\"inputs['english'].shape: {inputs['english'].shape}\")\n",
        "    print(f\"inputs['sinhala'].shape: {inputs['sinhala'].shape}\")\n",
        "    print(f\"targets.shape: {targets.shape}\")\n",
        "\n",
        "# Print a sample from the training dataset\n",
        "print(list(train_ds.as_numpy_iterator())[50])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bFhQBd1VClK5",
        "outputId": "65a9fda2-c27b-43cf-e376-8c58868da9c0"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs['english'].shape: (64, 20)\n",
            "inputs['sinhala'].shape: (64, 20)\n",
            "targets.shape: (64, 20)\n",
            "({'english': array([[  73,    2,    9, ...,    0,    0,    0],\n",
            "       [  10,  146,  276, ...,    0,    0,    0],\n",
            "       [ 108,   22,  108, ...,    0,    0,    0],\n",
            "       ...,\n",
            "       [2879,    0,    0, ...,    0,    0,    0],\n",
            "       [3563, 7704,    0, ...,    0,    0,    0],\n",
            "       [   7, 6249,   50, ...,    0,    0,    0]]), 'sinhala': array([[    2,    49,     5, ...,     0,     0,     0],\n",
            "       [    2,   144,   267, ...,     0,     0,     0],\n",
            "       [    2,    73,    73, ...,     0,     0,     0],\n",
            "       ...,\n",
            "       [    2, 13411,     3, ...,     0,     0,     0],\n",
            "       [    2, 11735, 14875, ...,     0,     0,     0],\n",
            "       [    2,   106,   733, ...,     0,     0,     0]])}, array([[   49,     5,   134, ...,     0,     0,     0],\n",
            "       [  144,   267,     6, ...,     0,     0,     0],\n",
            "       [   73,    73,    73, ...,     0,     0,     0],\n",
            "       ...,\n",
            "       [13411,     3,     0, ...,     0,     0,     0],\n",
            "       [11735, 14875,     3, ...,     0,     0,     0],\n",
            "       [  106,   733,    59, ...,     0,     0,     0]]))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Transformer Model"
      ],
      "metadata": {
        "id": "ejtSuoKm14Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Transformer Encoder Implemented as a Subclassed Layer"
      ],
      "metadata": {
        "id": "S5pRMk2sFRjl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        # Multi-head self-attention mechanism\n",
        "        self.attention = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim)\n",
        "        # Feed-forward neural network layers\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(dense_dim, activation=\"relu\"),\n",
        "             layers.Dense(embed_dim),]\n",
        "        )\n",
        "        # Layer normalization for the two sub-layers\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        if mask is not None:\n",
        "            mask = mask[:, tf.newaxis, :]\n",
        "        # Self-attention mechanism\n",
        "        attention_output = self.attention(\n",
        "            inputs, inputs, attention_mask=mask)\n",
        "        # Add and normalize the self-attention output with the input\n",
        "        proj_input = self.layernorm_1(inputs + attention_output)\n",
        "        # Feed-forward network processing\n",
        "        proj_output = self.dense_proj(proj_input)\n",
        "        # Add and normalize the output with the input and self-attention output\n",
        "        return self.layernorm_2(proj_input + proj_output)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"dense_dim\": self.dense_dim,\n",
        "        })\n",
        "        return config\n"
      ],
      "metadata": {
        "id": "k0mLqiIGFb56"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##The Transformer Decorder"
      ],
      "metadata": {
        "id": "3kZmzDRtGLgB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerDecoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        # Multi-head self-attention mechanism for decoder input\n",
        "        self.attention_1 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim)\n",
        "        # Multi-head attention mechanism for encoder-decoder attention\n",
        "        self.attention_2 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim)\n",
        "        # Feed-forward neural network layers\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(dense_dim, activation=\"relu\"),\n",
        "             layers.Dense(embed_dim),]\n",
        "        )\n",
        "        # Layer normalization for the three sub-layers\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "        self.layernorm_3 = layers.LayerNormalization()\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"dense_dim\": self.dense_dim,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    def get_causal_attention_mask(self, inputs):\n",
        "        # Create a causal attention mask to prevent attending to future tokens\n",
        "        input_shape = tf.shape(inputs)\n",
        "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
        "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
        "        j = tf.range(sequence_length)\n",
        "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
        "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
        "        mult = tf.concat(\n",
        "            [tf.expand_dims(batch_size, -1),\n",
        "             tf.constant([1, 1], dtype=tf.int32)], axis=0)\n",
        "        return tf.tile(mask, mult)\n",
        "\n",
        "    def call(self, inputs, encoder_outputs, mask=None):\n",
        "        # Create a causal mask for the decoder input\n",
        "        causal_mask = self.get_causal_attention_mask(inputs)\n",
        "        if mask is not None:\n",
        "            # Combine the input mask with the causal mask\n",
        "            padding_mask = tf.cast(\n",
        "                mask[:, tf.newaxis, :], dtype=\"int32\")\n",
        "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
        "        else:\n",
        "            padding_mask = mask\n",
        "        # Self-attention mechanism for decoder input\n",
        "        attention_output_1 = self.attention_1(\n",
        "            query=inputs,\n",
        "            value=inputs,\n",
        "            key=inputs,\n",
        "            attention_mask=causal_mask)\n",
        "        # Add and normalize the self-attention output with the input\n",
        "        attention_output_1 = self.layernorm_1(inputs + attention_output_1)\n",
        "        # Attention mechanism for encoder-decoder attention\n",
        "        attention_output_2 = self.attention_2(\n",
        "            query=attention_output_1,\n",
        "            value=encoder_outputs,\n",
        "            key=encoder_outputs,\n",
        "            attention_mask=padding_mask,\n",
        "        )\n",
        "        # Add and normalize the output with the input and attention output\n",
        "        attention_output_2 = self.layernorm_2(\n",
        "            attention_output_1 + attention_output_2)\n",
        "        # Feed-forward network processing\n",
        "        proj_output = self.dense_proj(attention_output_2)\n",
        "        # Add and normalize the output with the attention output and projection output\n",
        "        return self.layernorm_3(attention_output_2 + proj_output)\n"
      ],
      "metadata": {
        "id": "C_UpLwg9GPBz"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Positional Encoding"
      ],
      "metadata": {
        "id": "cy8-LkvxGpcH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Positional embedding layer for incorporating positional information into token embeddings\n",
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        # Embedding layer for token embeddings\n",
        "        self.token_embeddings = layers.Embedding(\n",
        "            input_dim=input_dim, output_dim=output_dim)\n",
        "        # Embedding layer for positional embeddings\n",
        "        self.position_embeddings = layers.Embedding(\n",
        "            input_dim=sequence_length, output_dim=output_dim)\n",
        "        self.sequence_length = sequence_length\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "    def call(self, inputs):\n",
        "        length = tf.shape(inputs)[-1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        # Generate token embeddings\n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "        # Generate positional embeddings\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        # Add positional embeddings to token embeddings\n",
        "        return embedded_tokens + embedded_positions\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        # Create a mask to indicate valid tokens\n",
        "        return tf.math.not_equal(inputs, 0)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(PositionalEmbedding, self).get_config()\n",
        "        config.update({\n",
        "            \"output_dim\": self.output_dim,\n",
        "            \"sequence_length\": self.sequence_length,\n",
        "            \"input_dim\": self.input_dim,\n",
        "        })\n",
        "        return config\n"
      ],
      "metadata": {
        "id": "tgaQw5MJGsve"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## End-to-End Transformer"
      ],
      "metadata": {
        "id": "UYIF93rNHGEH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define parameters for the Transformer model\n",
        "embed_dim = 256  # Dimensionality of the token embeddings\n",
        "dense_dim = 2048  # Dimensionality of the feed-forward layer in the transformer blocks\n",
        "num_heads = 8  # Number of attention heads\n",
        "\n",
        "# Define inputs for the encoder and decoder\n",
        "encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"english\")  # Input sequence for the encoder (English)\n",
        "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"sinhala\")  # Input sequence for the decoder (Sinhala)\n",
        "\n",
        "# Embedding and encoding for the encoder inputs\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)  # Add positional embedding\n",
        "encoder_outputs = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)  # Apply TransformerEncoder to encode the input sequence\n",
        "\n",
        "# Embedding, decoding, and output generation for the decoder inputs\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)  # Add positional embedding\n",
        "x = TransformerDecoder(embed_dim, dense_dim, num_heads)(x, encoder_outputs)  # Apply TransformerDecoder to decode the input sequence\n",
        "x = layers.Dropout(0.5)(x)  # Apply dropout regularization to prevent overfitting\n",
        "decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)  # Generate output probabilities using a dense layer with softmax activation\n",
        "\n",
        "# Define the end-to-end Transformer model\n",
        "transformer = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)  # Combine encoder and decoder inputs to create the Transformer model\n",
        "\n",
        "# Print model summary\n",
        "transformer.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWTq9X5LHJ18",
        "outputId": "12244af0-6cbd-44f6-a138-30efc84078b0"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " english (InputLayer)        [(None, None)]               0         []                            \n",
            "                                                                                                  \n",
            " sinhala (InputLayer)        [(None, None)]               0         []                            \n",
            "                                                                                                  \n",
            " positional_embedding (Posi  (None, None, 256)            3845120   ['english[0][0]']             \n",
            " tionalEmbedding)                                                                                 \n",
            "                                                                                                  \n",
            " positional_embedding_1 (Po  (None, None, 256)            3845120   ['sinhala[0][0]']             \n",
            " sitionalEmbedding)                                                                               \n",
            "                                                                                                  \n",
            " transformer_encoder (Trans  (None, None, 256)            3155456   ['positional_embedding[0][0]']\n",
            " formerEncoder)                                                                                   \n",
            "                                                                                                  \n",
            " transformer_decoder (Trans  (None, None, 256)            5259520   ['positional_embedding_1[0][0]\n",
            " formerDecoder)                                                     ',                            \n",
            "                                                                     'transformer_encoder[0][0]'] \n",
            "                                                                                                  \n",
            " dropout (Dropout)           (None, None, 256)            0         ['transformer_decoder[0][0]'] \n",
            "                                                                                                  \n",
            " dense_4 (Dense)             (None, None, 15000)          3855000   ['dropout[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 19960216 (76.14 MB)\n",
            "Trainable params: 19960216 (76.14 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the Sequence-to-Sequence Transformer"
      ],
      "metadata": {
        "id": "PomxLE0oH68i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the Transformer model with RMSprop optimizer and sparse categorical crossentropy loss\n",
        "transformer.compile(\n",
        "    optimizer=\"rmsprop\",\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"])\n",
        "\n",
        "# Train the Transformer model on the training dataset for 30 epochs with validation on the validation dataset\n",
        "transformer.fit(train_ds, epochs=30, validation_data=val_ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YjfvCl_tICL4",
        "outputId": "1a29318f-9154-4071-d6fa-2e1da02fd374"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "883/883 [==============================] - 74s 74ms/step - loss: 4.7054 - accuracy: 0.4006 - val_loss: 3.8492 - val_accuracy: 0.4683\n",
            "Epoch 2/30\n",
            "883/883 [==============================] - 60s 67ms/step - loss: 3.9322 - accuracy: 0.4657 - val_loss: 3.5169 - val_accuracy: 0.4941\n",
            "Epoch 3/30\n",
            "883/883 [==============================] - 60s 68ms/step - loss: 3.6283 - accuracy: 0.4948 - val_loss: 3.3687 - val_accuracy: 0.5092\n",
            "Epoch 4/30\n",
            "883/883 [==============================] - 60s 68ms/step - loss: 3.4406 - accuracy: 0.5151 - val_loss: 3.3379 - val_accuracy: 0.5200\n",
            "Epoch 5/30\n",
            "883/883 [==============================] - 60s 68ms/step - loss: 3.3055 - accuracy: 0.5324 - val_loss: 3.3223 - val_accuracy: 0.5167\n",
            "Epoch 6/30\n",
            "883/883 [==============================] - 60s 68ms/step - loss: 3.2046 - accuracy: 0.5456 - val_loss: 3.3776 - val_accuracy: 0.5193\n",
            "Epoch 7/30\n",
            "883/883 [==============================] - 61s 69ms/step - loss: 3.1169 - accuracy: 0.5588 - val_loss: 3.2901 - val_accuracy: 0.5317\n",
            "Epoch 8/30\n",
            "883/883 [==============================] - 60s 68ms/step - loss: 3.0486 - accuracy: 0.5699 - val_loss: 3.3407 - val_accuracy: 0.5328\n",
            "Epoch 9/30\n",
            "883/883 [==============================] - 60s 68ms/step - loss: 2.9891 - accuracy: 0.5797 - val_loss: 3.3428 - val_accuracy: 0.5342\n",
            "Epoch 10/30\n",
            "883/883 [==============================] - 60s 68ms/step - loss: 2.9356 - accuracy: 0.5894 - val_loss: 3.3597 - val_accuracy: 0.5309\n",
            "Epoch 11/30\n",
            "883/883 [==============================] - 60s 68ms/step - loss: 2.8925 - accuracy: 0.5964 - val_loss: 3.3841 - val_accuracy: 0.5364\n",
            "Epoch 12/30\n",
            "883/883 [==============================] - 65s 73ms/step - loss: 2.8482 - accuracy: 0.6049 - val_loss: 3.3972 - val_accuracy: 0.5318\n",
            "Epoch 13/30\n",
            "883/883 [==============================] - 61s 69ms/step - loss: 2.8079 - accuracy: 0.6119 - val_loss: 3.4173 - val_accuracy: 0.5350\n",
            "Epoch 14/30\n",
            "883/883 [==============================] - 61s 69ms/step - loss: 2.7715 - accuracy: 0.6175 - val_loss: 3.4501 - val_accuracy: 0.5367\n",
            "Epoch 15/30\n",
            "883/883 [==============================] - 64s 73ms/step - loss: 2.7432 - accuracy: 0.6232 - val_loss: 3.4776 - val_accuracy: 0.5310\n",
            "Epoch 16/30\n",
            "883/883 [==============================] - 60s 68ms/step - loss: 2.7129 - accuracy: 0.6276 - val_loss: 3.4633 - val_accuracy: 0.5344\n",
            "Epoch 17/30\n",
            "883/883 [==============================] - 60s 68ms/step - loss: 2.6853 - accuracy: 0.6327 - val_loss: 3.4788 - val_accuracy: 0.5352\n",
            "Epoch 18/30\n",
            "883/883 [==============================] - 60s 68ms/step - loss: 2.6640 - accuracy: 0.6361 - val_loss: 3.5224 - val_accuracy: 0.5296\n",
            "Epoch 19/30\n",
            "883/883 [==============================] - 64s 73ms/step - loss: 2.6402 - accuracy: 0.6403 - val_loss: 3.5337 - val_accuracy: 0.5328\n",
            "Epoch 20/30\n",
            "883/883 [==============================] - 64s 73ms/step - loss: 2.6193 - accuracy: 0.6442 - val_loss: 3.5326 - val_accuracy: 0.5374\n",
            "Epoch 21/30\n",
            "883/883 [==============================] - 61s 69ms/step - loss: 2.5939 - accuracy: 0.6487 - val_loss: 3.5550 - val_accuracy: 0.5348\n",
            "Epoch 22/30\n",
            "883/883 [==============================] - 60s 68ms/step - loss: 2.5732 - accuracy: 0.6518 - val_loss: 3.6123 - val_accuracy: 0.5239\n",
            "Epoch 23/30\n",
            "883/883 [==============================] - 61s 69ms/step - loss: 2.5511 - accuracy: 0.6554 - val_loss: 3.5751 - val_accuracy: 0.5336\n",
            "Epoch 24/30\n",
            "883/883 [==============================] - 60s 68ms/step - loss: 2.5334 - accuracy: 0.6585 - val_loss: 3.5760 - val_accuracy: 0.5354\n",
            "Epoch 25/30\n",
            "883/883 [==============================] - 60s 68ms/step - loss: 2.5107 - accuracy: 0.6623 - val_loss: 3.6354 - val_accuracy: 0.5296\n",
            "Epoch 26/30\n",
            "883/883 [==============================] - 61s 69ms/step - loss: 2.4905 - accuracy: 0.6661 - val_loss: 3.6519 - val_accuracy: 0.5337\n",
            "Epoch 27/30\n",
            "883/883 [==============================] - 60s 68ms/step - loss: 2.4724 - accuracy: 0.6694 - val_loss: 3.7071 - val_accuracy: 0.5336\n",
            "Epoch 28/30\n",
            "883/883 [==============================] - 60s 68ms/step - loss: 2.4546 - accuracy: 0.6727 - val_loss: 3.7153 - val_accuracy: 0.5319\n",
            "Epoch 29/30\n",
            "883/883 [==============================] - 65s 73ms/step - loss: 2.4369 - accuracy: 0.6749 - val_loss: 3.6975 - val_accuracy: 0.5360\n",
            "Epoch 30/30\n",
            "883/883 [==============================] - 60s 68ms/step - loss: 2.4197 - accuracy: 0.6778 - val_loss: 3.7181 - val_accuracy: 0.5317\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7cbb339dc5b0>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Testing the Trained Model"
      ],
      "metadata": {
        "id": "quS7lyie1q3j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Testing the Transformer on Test Dataset"
      ],
      "metadata": {
        "id": "xNHxElXQ0d_m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Get the vocabulary and create a lookup dictionary for the target language\n",
        "sin_vocab = target_vectorization.get_vocabulary()\n",
        "sin_index_lookup = dict(zip(range(len(sin_vocab)), sin_vocab))\n",
        "\n",
        "# Define the maximum length for decoding a sentence\n",
        "max_decoded_sentence_length = 20\n",
        "\n",
        "# Function to decode a sequence given an input sentence\n",
        "def decode_sequence(input_sentence):\n",
        "    # Vectorize the input sentence\n",
        "    tokenized_input_sentence = source_vectorization([input_sentence])\n",
        "\n",
        "    # Initialize the decoded sentence with a start token\n",
        "    decoded_sentence = \"[start]\"\n",
        "\n",
        "    # Loop over the maximum decoded sentence length\n",
        "    for i in range(max_decoded_sentence_length):\n",
        "        # Vectorize the current decoded sentence (excluding the last token)\n",
        "        tokenized_target_sentence = target_vectorization([decoded_sentence])[:, :-1]\n",
        "\n",
        "        # Get predictions from the transformer model\n",
        "        predictions = transformer([tokenized_input_sentence, tokenized_target_sentence])\n",
        "\n",
        "        # Sample the token index with the highest probability\n",
        "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
        "\n",
        "        # Get the corresponding token from the lookup dictionary\n",
        "        sampled_token = sin_index_lookup[sampled_token_index]\n",
        "\n",
        "        # Append the sampled token to the decoded sentence\n",
        "        decoded_sentence += \" \" + sampled_token\n",
        "\n",
        "        # Check if the end token is reached, and break the loop if so\n",
        "        if sampled_token == \"[end]\":\n",
        "            break\n",
        "\n",
        "    return decoded_sentence\n",
        "\n",
        "# Get a list of English sentences from the test pairs\n",
        "test_eng_texts = [pair[0] for pair in test_pairs]\n",
        "\n",
        "# Loop over a number of iterations to decode random sentences\n",
        "for _ in range(20):\n",
        "    # Choose a random input sentence from the list of English sentences\n",
        "    input_sentence = random.choice(test_eng_texts)\n",
        "\n",
        "    # Print the input sentence\n",
        "    print(\"-\")\n",
        "    print(\"English : \", input_sentence)\n",
        "\n",
        "    # Decode the input sentence and print the decoded sequence\n",
        "    print(\"Sinhala : \", decode_sequence(input_sentence))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VUfhdV_ptyMk",
        "outputId": "e2af249a-a5c4-45b7-9cbe-124ec1b6a02e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-\n",
            "English :  at once he called me and told taylor\n",
            "Sinhala :  [start] එක මට [UNK] එයාට කෝල් කරලා කිව්වා [end]\n",
            "-\n",
            "English :  these thoughts\n",
            "Sinhala :  [start] මේ [UNK] [end]\n",
            "-\n",
            "English :  ok if you want to go there you must go\n",
            "Sinhala :  [start] හරි ඔයාට යන්න ඕනි නම් [UNK] යන්න වෙනවා [end]\n",
            "-\n",
            "English :  grandma could not bear it anymore\n",
            "Sinhala :  [start] ආච්චි [UNK] තව උදව් වෙන්න බෑ [end]\n",
            "-\n",
            "English :  what is that  \n",
            "Sinhala :  [start] ඒ මොකක්ද [end]\n",
            "-\n",
            "English :  dee what's wrong  \n",
            "Sinhala :  [start] [UNK] මොකක්ද අවුල [end]\n",
            "-\n",
            "English :  we risked our lives for a thousand rupees\n",
            "Sinhala :  [start] අපි අපේ ජීවිත වෙනුවෙන් ජීවිත වලට දැම්මා ඒකට [end]\n",
            "-\n",
            "English :  jonah  \n",
            "Sinhala :  [start] [UNK] [end]\n",
            "-\n",
            "English :  handwriting analysts confirmed\n",
            "Sinhala :  [start] [UNK] වුනා [UNK] [end]\n",
            "-\n",
            "English :  people do a lot of things to cheat death doesn't he  \n",
            "Sinhala :  [start] මිනිස්සු ගොඩක් දේවල් වලට එයා කැමති වෙන්න [UNK] [end]\n",
            "-\n",
            "English :  it was my mistake\n",
            "Sinhala :  [start] ඒක මගේ වැරැද්ද [end]\n",
            "-\n",
            "English :  it is impossible\n",
            "Sinhala :  [start] ඒක කරන්න බැරි දෙයක් [end]\n",
            "-\n",
            "English :  not this is the color i used in high school\n",
            "Sinhala :  [start] නෑ මේ [UNK] මම [UNK] [UNK] තිබ්බේ [end]\n",
            "-\n",
            "English :  come on\n",
            "Sinhala :  [start] එන්න [end]\n",
            "-\n",
            "English :  see you\n",
            "Sinhala :  [start] බලන්න [end]\n",
            "-\n",
            "English :  what are you talking about  \n",
            "Sinhala :  [start] ඔයා මොනවාද මේ කියන්නේ [end]\n",
            "-\n",
            "English :  someone who has no loving feelings and is not afraid of losing a limb\n",
            "Sinhala :  [start] කෙනෙක් ආදරේ කරන කෙනෙක් නිසා ලැබෙන්නේ නැහැ [UNK] මට බය [UNK] විදියට කරලා නැහැ [end]\n",
            "-\n",
            "English :  what stupid bot\n",
            "Sinhala :  [start] මොකද මෝඩ තෝවූ [end]\n",
            "-\n",
            "English :  cancel the marriage how hard is that  \n",
            "Sinhala :  [start] ආන්ටි කසාදේ අනේ ඒක කොහොමද ඒක [end]\n",
            "-\n",
            "English :  everyone thought that pathan's chapter was over\n",
            "Sinhala :  [start] හැමෝම හිතුවේ ඒ [UNK] ට මුලින්ම වෙනවා කියලා [end]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing the Transformer on User Inputs"
      ],
      "metadata": {
        "id": "vFit8AvC-70C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get English input from the user\n",
        "input_sentence = input(\"Enter an English sentence: \")\n",
        "\n",
        "# Translate the input sentence to Sinhala\n",
        "translated_sentence = decode_sequence(input_sentence)\n",
        "\n",
        "# Print the translated sentence\n",
        "print(\"Sinhala translation:\", translated_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J2dcRh1T07q5",
        "outputId": "b53ce238-8717-4afb-acb6-ceba4aadf642"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter an English sentence: i'm not going.\n",
            "Sinhala translation: [start] මම යන්නේ නැහැ [end]\n"
          ]
        }
      ]
    }
  ]
}